## Phase 1: Numeric Foundations
- Chapter 01 → Dot product
- Chapter 02 → Vector arithmetic (add, scale, norm)
- Chapter 03 → Matrix representation & indexing
- Chapter 04 → Matrix-vector multiply
- Chapter 05 → Matrix-matrix multiply

## Phase 2: Distance & Loss
- Chapter 06 → Euclidean & cosine distance
- Chapter 07 → MSE and MAE loss functions
- Chapter 08 → Binary cross-entropy loss

## Phase 3: Calculus by Hand
- Chapter 09 → Numerical differentiation (finite difference)
- Chapter 10 → Gradient of a scalar function over a vector
- Chapter 11 → Gradient descent on a simple function

## Phase 4: The Neuron
- Chapter 12 → Single neuron (linear: w·x + b)
- Chapter 13 → Activation functions (sigmoid, tanh, ReLU)
- Chapter 14 → Single neuron with activation + MSE loss
- Chapter 15 → Gradient descent to train one neuron

## Phase 5: The Network
- Chapter 16 → Layer abstraction (Dense layer)
- Chapter 17 → Forward pass through an MLP
- Chapter 18 → Loss over a batch

## Phase 6: Backpropagation
- Chapter 19 → Chain rule, by hand on paper first
- Chapter 20 → Backprop through one layer
- Chapter 21 → Backprop through the full MLP
- Chapter 22 → Weight update loop (SGD)

## Phase 7: Training a Real Model
- Chapter 23 → Train MLP on XOR
- Chapter 24 → Train MLP on a real dataset (e.g. Iris)
- Chapter 25 → Momentum, learning rate decay